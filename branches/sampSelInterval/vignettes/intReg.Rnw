\documentclass[a4paper]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[bookmarks=TRUE,
            colorlinks,
            pdfpagemode=none,
            pdfstartview=FitH,
            citecolor=black,
            filecolor=black,
            linkcolor=black,
            urlcolor=black,
            ]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bbm}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\loglik}{\ell}% log likelihood
\newcommand{\var}{\mathrm{Var}\,}
\renewcommand*{\vec}[1]{\boldsymbol{#1}}% vector

\title{Interval Regression with Sample Selection}
\author{G\'eraldine Henningsen, Arne Henningsen, Sebastian Petersen}

\begin{document}
\SweaveOpts{concordance=TRUE}
%\VignetteIndexEntry{Sample Selection Interval Regression}
%\VignetteKeyword{models}
%\VignetteKeyword{regression}

\maketitle

\section{Model Specification}

The general specification
of an interval regression model with sample selection is:
\begin{align}
   y_i^{S*} &= {\vec{\beta}^S}' \vec{x}_i^S + \varepsilon_i^S
   \label{eq:selection*}
   \\
   y_i^S & = 
   \begin{cases}
      0 & \quad \text{if } y_i^{S*} < 0
      \label{eq:selection}
      \\
      1 & \quad \text{otherwise}
   \end{cases}
   \\
   y_i^{O*} &= {\vec{\beta}^O}' \vec{x}_i^O + \varepsilon_i^O
   \label{eq:outcome*}
   \\
   y_i^O &= 
   \begin{cases}
      \text{unknown} & \quad \text{if } y_i^{S} = 0\\
      1 & \quad \text{if } \alpha_0 < y_i^{O*} \leq \alpha_1
         \text{ and } y_i^{S} = 1\\
      2 & \quad \text{if } \alpha_1 < y_i^{O*} \leq \alpha_2
         \text{ and } y_i^{S} = 1\\
      \vdots & \\
      M & \quad \text{if } \alpha_{M-1} < y_i^{O*} \leq \alpha_M
         \text{ and } y_i^{S} = 1
   \end{cases}
   \\
   \left(
      \begin{array}{c} \varepsilon_i^S \\ \varepsilon_i^O \end{array}
   \right)
   &\sim N_2 \left( \left( \begin{array}{c} 0 \\ 0 \end{array} \right),
      \left[ \begin{matrix}
         1 & \rho \sigma_2 \\ 
         \rho \sigma_2 & \sigma_2^2
      \end{matrix} \right]
   \right),
\end{align}
where subscript~$i$ indicates the observation,
$y_i^{O*}$~is a latent outcome variable,
$y_i^O$~is a partially observed categorical variable
that indicates in which interval $y_i^{O*}$ lies,
$M$~is the number of intervals,
$\alpha_0 , \ldots , \alpha_M$~are the boundaries of the intervals
(whereas frequently but not necessarily
$\alpha_0 = - \infty$ and $\alpha_M = \infty$),
$y_i^S$~is a binary variable
that indicates whether $y_i^O$ is observed,
$y_i^{S*}$ is a latent variable
that indicates the ``tendency'' that $y_i^S$ is one,
$\vec{x}_i^S$ and $\vec{x}_i^O$ are (column) vectors of explanatory variables
for the selection equation and outcome equation, respectively,
$\varepsilon_i^S$ and $\varepsilon_i^O$
are random disturbance terms
that have a joint bivariate normal distribution,
and $\vec{\beta}^S$ and $\vec{\beta}^O$ are (column) vectors
and $\rho$ and $\sigma_2$ are scalars of unknown model parameters.


\section{Log-Likelihood Function}

The probability that $y_i^O$ is unobserved is:
\begin{align}
P \left( y_i^S = 0 \right)
& = P \left( y_i^{S*} \leq 0 \right)\\
& = P \left( {\vec{\beta}^S}' \vec{x}_i^S + \varepsilon_i^S \leq 0 \right)\\
& = P \left( \varepsilon_i^S \leq - {\vec{\beta}^S}' \vec{x}_i^S \right)
\end{align}

The probability that $y_i^O$ is observed and indicates
that $y_i^{O*}$ lies in the $m$th interval is:
\begin{align}
P \left( y_i^S = 1 \wedge y_i^O = m \right)
& = P \left( y_i^{S*} > 0 \wedge
   \alpha_{m} < y_i^{O*} \leq \alpha_{m+1} \right)\\
& = P \left( {\vec{\beta}^S}' \vec{x}_i^S + \varepsilon_i^S > 0 \wedge
   \alpha_{m} < {\vec{\beta}^O}' \vec{x}_i^O + \varepsilon_i^O
   \leq \alpha_{m+1} \right)\\
& = P \left( \varepsilon_i^S > - {\vec{\beta}^S}' \vec{x}_i^S \wedge
   \alpha_{m} - {\vec{\beta}^O}' \vec{x}_i^O < \varepsilon_i^O
   \leq \alpha_{m+1} - {\vec{\beta}^O}' \vec{x}_i^O \right)
\end{align}

The log-likelihood contribution of the $i$th observation is:
\begin{align}
\ell_i = & ( 1 - y_i^S )
   \ln \left[ \Phi \left( - {\vec{\beta}^S}' \vec{x}_i^S \right) \right]\\
   & + \sum_{m=1}^M y_i^S ( y_i^O = m ) \ln \left[
      \Phi_2 \left(
         \frac{\alpha_{m+1} - {\vec{\beta}^O}' \vec{x}_i^O}{\sigma_2},
         {\vec{\beta}^S}' \vec{x}_i^S, - \rho \right) \right.\\
      & \qquad \left. - \Phi_2 \left(
         \frac{\alpha_{m} - {\vec{\beta}^O}' \vec{x}_i^O}{\sigma_2},
         {\vec{\beta}^S}' \vec{x}_i^S, - \rho \right) 
   \right],
\end{align}
where $\Phi(.)$ indicates the cumulative distribution function
of the univariate standard normal distribution
and $\Phi_2(.)$ indicates the cumulative distribution function
of the bivariate standard normal distribution.


\section{Gradients of the CDF of the bivariate normal distribution}

In order to facilitate the calculation of the gradients
of the log-likelihood function,
we calculate the partial derivatives
of the cumulative distribution function~(CDF)
of the bivariate standard normal distribution:

\begin{align}
\Phi_2 ( x1, x2 , \rho ) 
& = \int_{- \infty}^{x_2} \int_{- \infty}^{x_1}
   \phi_2 ( a_1 , a_2 , \rho ) \; d a_1 \; d a_2
   \label{eq:biv}
\end{align}


\subsection{Gradients with respect to the limits ($x_1$ and $x_2$)}

\begin{align}
\frac{\partial \Phi_2 ( x1, x2 , \rho )}{\partial x_2} 
& = \int_{- \infty}^{x_1} \phi_2( a_1 , x_2 , \rho ) \; d a_1
   \label{eq:derivBivFirst}\\
& = \int_{- \infty}^{x_1} \phi( a_1 | x_2, \rho ) \phi( x_2 ) \; d a_1
   \label{eq:derivBivCondFirst}\\
& = \int_{- \infty}^{x_1}
   \tilde{\phi} \left( a_1, \rho x_2, 1 - \rho^2 \right) \phi( x_2 ) \; d a_1
   \label{eq:derivBivCondNonNormal}\\
& = \int_{- \infty}^{x_1}
   \phi \left( \frac{a_1 - \rho x_2}{\sqrt{1 - \rho^2}} \right)
   \left( \sqrt{1 - \rho^2} \right)^{-1} \phi( x_2 ) \; d a_1
   \label{eq:derivBivCondFinal}\\
& = \int_{- \infty}^{x_1}
      \phi \left( \frac{a_1 - \rho x_2}{\sqrt{1 - \rho^2}} \right)
      \left( \sqrt{1 - \rho^2} \right)^{-1} d a_1 \; \phi( x_2 )
   \label{eq:derivBivCondX2out}\\
& = \int_{- \infty}^{\frac{x_1 - \rho x_2}{\sqrt{1 - \rho^2}}}
      \phi ( a_1 ) \; d a_1 \; \phi( x_2 )
   \label{eq:derivBivBorders}\\
& = \Phi \left( \frac{x_1 - \rho x_2}{\sqrt{1 - \rho^2}} \right)
   \phi( x_2 ),
   \label{eq:derivBivFinal}
\end{align}
where $\tilde{\phi}( \; , \mu , \sigma^2 )$
indicates the density function of a normal distribution
with mean~$\mu$ and variance~$\sigma^2$.

In the following, we check the above derivations by a simple numerical example:
<<>>=
library( "mvtnorm" )
library( "maxLik" )
x1 <- 0.4
x2 <- -0.3
rho <- -0.6
sigma <-  matrix( c( 1, rho, rho, 1 ), nrow = 2 )
@

First, we check whether $\phi_2 ( x_1 , x_2 , \rho )$
(equation~\ref{eq:derivBivFirst})
is equal to $\tilde{\phi} \left( x_1, \rho x_2, 1 - \rho^2 \right) \phi( x_2 )$
(equation~\ref{eq:derivBivCondNonNormal})
and equal to $\phi \left( ( x_1 - \rho x_2 ) / ( \sqrt{1 - \rho^2} ) \right)
   \left( \sqrt{1 - \rho^2} \right)^{-1} \phi( x_2 )$
(equations~\ref{eq:derivBivCondFinal} and~\ref{eq:derivBivCondX2out}):
<<>>=
dens <- dmvnorm( c( x1, x2 ), sigma = sigma )
print( dens )
all.equal( dens, dnorm( x1, rho * x2, sqrt( 1 - rho^2 ) ) * dnorm(x2) )
all.equal( dens, ( dnorm( ( x1 - rho * x2 ) / sqrt( 1 - rho^2 ) ) /
   sqrt( 1 - rho^2 ) ) * dnorm(x2) )
@

In the following, we will numerically calculate the derivative
of the cumulative distribution function of the bivaraite normal distribution
(equation~\ref{eq:biv}) with respect to~$x_2$
and check wehther this partial derivative
is equal to the right-hand sides of equations~\ref{eq:derivBivFirst},
\ref{eq:derivBivCondFinal}, \ref{eq:derivBivCondX2out}, 
and~\ref{eq:derivBivFinal}:
<<>>=
funX2 <- function( a2 ) {
   prob <- pmvnorm( upper = c( x1, a2 ), sigma = sigma )
   return( prob )
}
grad <- c( numericGradient( funX2, x2 ) )
print( grad )

funX1 <- function( a1 ) {
   dens <- rep( NA, length( a1 ) )
   for( i in 1:length( a1 ) ) {
      dens[i] <- dmvnorm( c( a1[i], x2 ), sigma = sigma )
   }
   return( dens )
}
all.equal( grad, integrate( funX1, lower = -Inf, upper = x1 )$value )


funX1a <- function( a1 ) {
   dens <- rep( NA, length( a1 ) )
   for( i in 1:length( a1 ) ) {
      dens[i] <- ( dnorm( ( a1[i] - rho * x2 ) / sqrt( 1 - rho^2 ) ) /
            sqrt(1-rho^2) ) * dnorm(x2)
   }
   return( dens )
}
all.equal( grad, integrate( funX1a, lower = -Inf, upper = x1 )$value )

funX1b <- function( a1 ) {
   dens <- rep( NA, length( a1 ) )
   for( i in 1:length( a1 ) ) {
      dens[i] <- dnorm( ( a1[i] - rho * x2 ) / sqrt( 1 - rho^2 ) ) /
         sqrt(1-rho^2)
   }
   return( dens )
}
all.equal( grad,
   integrate( funX1b, lower = -Inf, upper = x1 )$value * dnorm(x2) )

all.equal( grad,
   pnorm( ( x1 - rho * x2 ) / sqrt( 1 - rho^2 ) ) * dnorm( x2 ) )
@


\subsection{Gradients with respect to the coefficient of correlation ($\rho$)}

\begin{align}
\frac{\partial \Phi_2 ( a1, a2 , \rho )}{\partial \rho}
= \, & \int_{- \infty}^{x_2} \int_{- \infty}^{x_1} 
         \frac{\partial \phi_2 (a_1, a_2, \rho)}{\partial \rho} 
      \; d a_1 \; d a_2
      \label{eq:derivBivrho_start}
\\ \nonumber
= \, & \int_{- \infty}^{x_2} \int_{- \infty}^{x_1}
         \frac{\partial}{\partial \rho} \Bigg[
         \frac{1}{2 \pi \sqrt{(1-\rho^2)}} \cdot
         \\
         & \exp \left( -\frac{1}{2(1-\rho^2)} \left (a_1^2 -2\rho a_1 a_2 + a_2^2 \right ) \right ) \Bigg]
      \; d a_1 \; d a_2
\\ \nonumber
= \, &  \int_{- \infty}^{x_2} \int_{- \infty}^{x_1}
         \frac{\rho}{2 \pi (1-\rho^2)^{3/2}} \cdot
         \\
         & \exp \left( -\frac{1}{2(1-\rho^2)} \left (a_1^2 -2\rho a_1 a_2 + a_2^2 \right ) \right ) \Bigg]
      \; d a_1 \; d a_2
      \label{eq:derivBivrho_final}
\end{align}

In the following, we will numerically calculate the derivative
of the cumulative distribution function of the bivariate normal distribution
(equation~\ref{eq:derivBivrho_start}) with respect to~$\rho$
and check wehther this partial derivative
is equal to the right-hand sides of equation~\ref{eq:derivBivrho_final}:
<<>>=

# Numerical gradient for rho
funrho <- function( p ) {
   prob <- dmvnorm( x = c( x1, x2 ),
      sigma = matrix( c( 1, p, p, 1 ), nrow = 2 ) )
   return( prob )
}
grad <- c( numericGradient( funrho, rho ) )
print( grad )

# Comparison with analytical gradient for rho
all.equal( grad, 
   ( rho /(2*pi*(1 - rho^2)^(3/2)) ) * 
      exp((-1/(2*(1 - rho^2))) * 
            (x1^2 - 2 * rho * x1 * x2 + x2^2)) )
@

\section{Gradients of the Log-Likelihood Function}


\end{document}
