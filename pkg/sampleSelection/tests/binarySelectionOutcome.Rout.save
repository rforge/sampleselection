
R version 3.1.0 (2014-04-10) -- "Spring Dance"
Copyright (C) 2014 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> options( digits = 3 )
> 
> ## Leeman Lucas (and many others): binary outcome
> 
> set.seed(0)
> N <- 500
> rho <- 0.7
> library( "mvtnorm" )
> eps <- rmvnorm(N, c(0,0), matrix(c(1,rho,rho,1), 2, 2) )
> xs <- runif(N)
> ysX <- 3*xs + eps[,1]
> ys <- ysX > 0
> xo <- runif(N)
> yoX <- -1 + 2*xo + eps[,2]
> yo <- factor((yoX > 0)*(ys > 0))
>                            # binary outcome, only observable if ys>0
> print(table(ys, yo, exclude=NULL))
       yo
ys        0   1 <NA>
  FALSE  74   0    0
  TRUE  202 224    0
  <NA>    0   0    0
> library( "sampleSelection" )
Loading required package: maxLik
Loading required package: miscTools

Please cite the 'maxLik' package as:
Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.

If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
https://r-forge.r-project.org/projects/maxlik/
> 
> # estimation with BHHH method
> ss <- selection( ys ~ xs, yo ~ xo, steptol = 1e-12 )
> print( ss )

Call:
 selection(selection = ys ~ xs, outcome = yo ~ xo, steptol = 1e-12) 

Coefficients:
S:(Intercept)           S:xs  O:(Intercept)           O:xo            rho  
       -0.130          3.347         -1.054          1.997          0.803  

> summary( ss )
--------------------------------------------
Tobit 2 model (sample selection model)
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -398 
500 observations (74 censored and 426 observed)
5 free parameters (df = 495)
Probit selection equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -0.130      0.130   -1.00    0.32    
xs             3.347      0.392    8.54  <2e-16 ***
Outcome equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -1.054      0.124   -8.47  <2e-16 ***
xo             1.997      0.220    9.07  <2e-16 ***
Error terms:
    Estimate Std. error t value Pr(> t)    
rho    0.803      0.158    5.07   4e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> coef( ss )
S:(Intercept)           S:xs  O:(Intercept)           O:xo            rho  
       -0.130          3.347         -1.054          1.997          0.803  
> coef( ss, part = "outcome" )
(Intercept)           xo  
      -1.05         2.00  
> coef( summary( ss ) )
            Estimate Std. error t value  Pr(> t)
(Intercept)   -0.130      0.130  -0.998 3.18e-01
xs             3.347      0.392   8.536 1.39e-17
(Intercept)   -1.054      0.124  -8.469 2.47e-17
xo             1.997      0.220   9.068 1.21e-19
rho            0.803      0.158   5.068 4.02e-07
> coef( summary( ss ), part = "outcome" )
            Estimate Std. error t value  Pr(> t)
(Intercept)    -1.05      0.124   -8.47 2.47e-17
xo              2.00      0.220    9.07 1.21e-19
> stdEr( ss )
(Intercept)          xs (Intercept)          xo         rho 
      0.130       0.392       0.124       0.220       0.158 
> vcov( ss )
            (Intercept)        xs (Intercept)       xo      rho
(Intercept)     0.01689 -4.00e-02    5.50e-04 -0.00248  0.00694
xs             -0.03999  1.54e-01   -3.29e-05  0.00988 -0.01613
(Intercept)     0.00055 -3.29e-05    1.55e-02 -0.02182 -0.00438
xo             -0.00248  9.88e-03   -2.18e-02  0.04852 -0.00517
rho             0.00694 -1.61e-02   -4.38e-03 -0.00517  0.02507
> vcov( ss, part = "outcome" )
            (Intercept)      xo
(Intercept)      0.0155 -0.0218
xo              -0.0218  0.0485
> nobs( ss )
[1] 500
> nObs( ss )
[1] 500
> round( fitted( ss ), 3 )
     1      2      3      4      5      6      7      8      9     10     11 
-0.414 -0.004 -0.908  0.643 -0.209  0.919     NA  0.755 -1.017 -0.671 -0.287 
    12     13     14     15     16     17     18     19     20     21     22 
-0.440  0.001  0.400  0.850     NA -0.791  0.161  0.927 -0.865  0.734 -0.386 
    23     24     25     26     27     28     29     30     31     32     33 
-0.111 -0.701 -0.352 -0.702  0.250 -0.583  0.738     NA  0.791 -0.060     NA 
    34     35     36     37     38     39     40     41     42     43     44 
    NA -0.625 -0.445  0.670 -0.663  0.645  0.837 -0.506  0.623     NA  0.458 
    45     46     47     48     49     50     51     52     53     54     55 
-0.163  0.854  0.215 -0.727 -0.242  0.903 -0.761     NA -0.662     NA  0.529 
    56     57     58     59     60     61     62     63     64     65     66 
 0.411  0.447     NA     NA -0.144 -0.547     NA -0.185  0.486 -0.796  0.911 
    67     68     69     70     71     72     73     74     75     76     77 
 0.572  0.906 -0.982  0.079 -0.252 -0.141 -0.249  0.089 -0.535  0.692  0.929 
    78     79     80     81     82     83     84     85     86     87     88 
 0.804  0.916  0.261 -0.450  0.678  0.286 -0.077  0.201  0.325  0.288     NA 
    89     90     91     92     93     94     95     96     97     98     99 
 0.319  0.943 -0.554 -0.099 -1.024 -0.551 -0.402  0.288  0.441 -0.370 -0.495 
   100    101    102    103    104    105    106    107    108    109    110 
 0.418  0.198  0.211 -0.623  0.755     NA -0.718  0.310 -0.827     NA  0.826 
   111    112    113    114    115    116    117    118    119    120    121 
-0.757  0.122  0.700 -0.699 -0.114     NA -0.856 -0.474 -0.564  0.389  0.017 
   122    123    124    125    126    127    128    129    130    131    132 
-0.457     NA  0.764 -0.989  0.137 -0.753  0.099 -0.147     NA -0.003     NA 
   133    134    135    136    137    138    139    140    141    142    143 
 0.801     NA -0.566  0.696 -0.690 -0.874 -0.577  0.666  0.609 -0.330 -0.679 
   144    145    146    147    148    149    150    151    152    153    154 
    NA -0.174 -0.102     NA -0.876     NA  0.215 -0.658 -0.532 -0.418     NA 
   155    156    157    158    159    160    161    162    163    164    165 
-0.950     NA  0.089 -0.981 -1.018 -0.662 -0.373 -0.488 -0.682 -0.646     NA 
   166    167    168    169    170    171    172    173    174    175    176 
-0.243  0.678 -0.503  0.449 -0.374  0.863  0.201  0.634 -0.810  0.056  0.496 
   177    178    179    180    181    182    183    184    185    186    187 
-0.536 -0.058  0.113 -0.086  0.161 -1.013 -0.290 -0.292 -0.920     NA  0.473 
   188    189    190    191    192    193    194    195    196    197    198 
 0.547 -0.647  0.779     NA     NA -0.029 -0.789 -0.379 -0.629     NA     NA 
   199    200    201    202    203    204    205    206    207    208    209 
-0.383 -0.765     NA -0.946  0.172  0.515  0.103 -0.001 -0.234  0.405  0.711 
   210    211    212    213    214    215    216    217    218    219    220 
-0.826     NA -0.322 -0.547 -0.813  0.110     NA  0.104  0.125  0.897 -0.563 
   221    222    223    224    225    226    227    228    229    230    231 
-0.101 -0.734  0.522 -0.863     NA -0.903     NA -0.667  0.373  0.855  0.427 
   232    233    234    235    236    237    238    239    240    241    242 
-0.543     NA -0.789 -0.694 -0.895 -0.513 -0.175  0.367     NA -0.754  0.143 
   243    244    245    246    247    248    249    250    251    252    253 
 0.725 -0.278     NA  0.558 -0.819  0.472 -0.802  0.652  0.018  0.136  0.669 
   254    255    256    257    258    259    260    261    262    263    264 
 0.678 -0.936     NA -0.370  0.593  0.003 -0.181 -0.942 -0.559 -0.259 -0.832 
   265    266    267    268    269    270    271    272    273    274    275 
    NA     NA  0.644     NA     NA  0.379 -0.864 -0.097 -0.800 -0.527 -0.601 
   276    277    278    279    280    281    282    283    284    285    286 
-0.686 -1.020 -0.492 -0.813 -0.243  0.522 -0.843 -0.372 -0.388     NA     NA 
   287    288    289    290    291    292    293    294    295    296    297 
 0.518  0.539 -0.607 -0.123 -0.411  0.033  0.886 -0.756 -0.328     NA  0.117 
   298    299    300    301    302    303    304    305    306    307    308 
 0.024  0.691  0.482 -0.154  0.821  0.061  0.086 -0.840 -0.278  0.898 -0.612 
   309    310    311    312    313    314    315    316    317    318    319 
-0.615 -0.491 -0.816     NA -0.486  0.232  0.864 -0.912 -0.979 -0.974 -0.222 
   320    321    322    323    324    325    326    327    328    329    330 
-0.551 -0.838  0.665 -0.752  0.042 -0.034     NA  0.310  0.441  0.848 -0.330 
   331    332    333    334    335    336    337    338    339    340    341 
-0.363 -0.147     NA  0.002 -1.049  0.667  0.058     NA  0.844  0.744  0.332 
   342    343    344    345    346    347    348    349    350    351    352 
    NA -0.129     NA -0.177  0.883 -0.496  0.902  0.310  0.759  0.284 -0.229 
   353    354    355    356    357    358    359    360    361    362    363 
 0.806 -0.756 -1.020  0.097     NA -0.476 -0.502  0.161     NA -0.473     NA 
   364    365    366    367    368    369    370    371    372    373    374 
 0.587     NA  0.915  0.175  0.608 -0.582  0.485 -0.937 -0.026  0.665  0.914 
   375    376    377    378    379    380    381    382    383    384    385 
-0.262 -0.119 -0.214  0.775 -0.091  0.828 -0.806  0.072  0.406     NA  0.201 
   386    387    388    389    390    391    392    393    394    395    396 
    NA -0.883  0.296     NA -0.862 -0.258     NA  0.533  0.193 -0.245 -1.049 
   397    398    399    400    401    402    403    404    405    406    407 
 0.161  0.129  0.764     NA     NA -0.545     NA -0.005 -0.147 -0.012 -0.883 
   408    409    410    411    412    413    414    415    416    417    418 
 0.139  0.311  0.043     NA -0.463 -0.486     NA -0.719  0.778  0.925  0.439 
   419    420    421    422    423    424    425    426    427    428    429 
-0.071  0.809  0.178 -0.272 -0.662  0.384  0.812  0.329  0.720 -0.598  0.037 
   430    431    432    433    434    435    436    437    438    439    440 
-0.641  0.018  0.882  0.550 -0.647 -0.254  0.602 -0.173 -0.265 -0.993 -0.783 
   441    442    443    444    445    446    447    448    449    450    451 
 0.298 -0.930  0.938 -0.827     NA -1.003 -0.838 -0.147  0.544  0.613     NA 
   452    453    454    455    456    457    458    459    460    461    462 
-0.489  0.230     NA  0.018 -0.193  0.392 -0.707 -0.415 -0.279     NA -0.499 
   463    464    465    466    467    468    469    470    471    472    473 
 0.375 -0.398  0.147 -0.008  0.664  0.010 -0.832     NA -0.929  0.354     NA 
   474    475    476    477    478    479    480    481    482    483    484 
 0.506  0.512  0.049  0.654 -0.635 -0.595     NA -0.166  0.264 -0.899 -0.369 
   485    486    487    488    489    490    491    492    493    494    495 
 0.558 -0.357 -0.950  0.670  0.058     NA  0.466  0.035 -0.100 -0.360     NA 
   496    497    498    499    500 
 0.845 -0.532 -0.841 -0.548 -0.693 
> all.equal( fitted( ss ), fitted( ss, part = "outcome" ) )
[1] TRUE
> round( fitted( ss, part = "selection" ), 3 )
    1     2     3     4     5     6     7     8     9    10    11    12    13 
0.878 0.997 0.999 0.997 0.909 0.696 0.558 0.966 0.542 0.949 0.927 0.513 0.969 
   14    15    16    17    18    19    20    21    22    23    24    25    26 
0.994 0.970 0.486 0.884 0.589 0.480 0.996 0.832 0.804 0.996 0.896 0.462 0.518 
   27    28    29    30    31    32    33    34    35    36    37    38    39 
0.990 0.546 0.542 0.977 0.895 0.905 0.555 0.787 0.994 0.570 0.787 0.628 0.982 
   40    41    42    43    44    45    46    47    48    49    50    51    52 
0.999 0.719 0.868 0.605 0.994 0.536 0.994 0.612 0.993 0.860 0.973 0.998 0.588 
   53    54    55    56    57    58    59    60    61    62    63    64    65 
0.988 0.653 0.772 0.993 0.969 0.575 0.656 0.987 0.780 0.806 0.960 0.847 0.999 
   66    67    68    69    70    71    72    73    74    75    76    77    78 
0.722 0.968 0.892 0.953 0.995 0.463 0.983 0.998 0.892 0.998 0.995 0.518 0.999 
   79    80    81    82    83    84    85    86    87    88    89    90    91 
0.938 0.938 0.707 0.984 0.956 0.972 0.816 0.663 0.997 0.678 0.967 0.999 0.951 
   92    93    94    95    96    97    98    99   100   101   102   103   104 
0.995 0.980 0.993 0.494 0.959 0.607 0.985 0.998 0.960 0.964 0.945 0.981 0.967 
  105   106   107   108   109   110   111   112   113   114   115   116   117 
0.555 0.653 0.999 0.999 0.804 0.994 0.985 0.887 0.885 0.998 0.996 0.776 0.683 
  118   119   120   121   122   123   124   125   126   127   128   129   130 
0.999 0.996 0.849 0.996 0.462 0.722 0.998 0.513 0.997 0.981 0.727 0.461 0.554 
  131   132   133   134   135   136   137   138   139   140   141   142   143 
0.993 0.493 0.993 0.883 0.994 0.493 0.863 0.714 0.999 0.961 0.911 0.995 0.664 
  144   145   146   147   148   149   150   151   152   153   154   155   156 
0.712 0.850 0.916 0.585 0.970 0.582 0.999 0.965 0.668 0.946 0.521 0.804 0.492 
  157   158   159   160   161   162   163   164   165   166   167   168   169 
0.895 0.988 0.909 0.928 0.873 0.993 0.999 0.912 0.787 0.980 0.992 0.967 0.864 
  170   171   172   173   174   175   176   177   178   179   180   181   182 
0.942 0.638 0.777 0.674 0.998 0.960 0.845 0.794 0.991 0.960 0.961 0.977 0.999 
  183   184   185   186   187   188   189   190   191   192   193   194   195 
0.993 0.992 0.649 0.865 0.998 0.937 0.485 0.916 0.485 0.473 0.997 0.999 0.998 
  196   197   198   199   200   201   202   203   204   205   206   207   208 
0.997 0.612 0.730 0.985 0.979 0.570 0.969 0.999 0.618 0.948 0.999 0.911 0.888 
  209   210   211   212   213   214   215   216   217   218   219   220   221 
0.863 0.963 0.589 0.981 0.995 0.658 0.938 0.631 0.985 0.930 0.714 0.988 0.838 
  222   223   224   225   226   227   228   229   230   231   232   233   234 
0.950 0.968 0.992 0.501 0.976 0.670 0.987 0.775 0.555 0.988 0.991 0.670 0.722 
  235   236   237   238   239   240   241   242   243   244   245   246   247 
0.629 0.827 0.981 0.920 0.730 0.515 0.996 0.921 0.508 0.835 0.852 0.746 0.716 
  248   249   250   251   252   253   254   255   256   257   258   259   260 
0.799 0.809 0.998 0.992 0.586 0.992 0.896 0.729 0.457 0.489 0.760 0.728 0.816 
  261   262   263   264   265   266   267   268   269   270   271   272   273 
0.998 0.999 0.935 0.667 0.692 0.695 0.940 0.556 0.994 0.963 0.999 0.997 0.987 
  274   275   276   277   278   279   280   281   282   283   284   285   286 
0.601 0.659 0.839 0.999 0.998 0.952 0.757 0.998 0.992 0.998 0.980 0.476 0.576 
  287   288   289   290   291   292   293   294   295   296   297   298   299 
0.999 0.998 0.919 0.994 0.706 0.820 0.950 0.685 0.951 0.850 0.943 0.994 0.911 
  300   301   302   303   304   305   306   307   308   309   310   311   312 
0.796 0.999 0.489 0.997 0.912 0.976 0.993 0.791 0.611 0.998 0.890 0.992 0.595 
  313   314   315   316   317   318   319   320   321   322   323   324   325 
0.842 0.955 0.820 0.914 0.889 0.933 0.910 0.777 0.980 0.749 0.995 0.995 0.967 
  326   327   328   329   330   331   332   333   334   335   336   337   338 
0.954 0.510 0.902 0.978 0.449 0.997 0.583 0.576 0.993 0.488 0.845 0.994 0.599 
  339   340   341   342   343   344   345   346   347   348   349   350   351 
0.851 0.835 0.986 0.854 0.891 0.816 0.999 0.985 0.998 0.999 0.991 0.996 0.995 
  352   353   354   355   356   357   358   359   360   361   362   363   364 
0.998 0.996 0.867 0.997 0.995 0.679 0.662 0.999 0.913 0.553 0.999 0.469 0.937 
  365   366   367   368   369   370   371   372   373   374   375   376   377 
0.903 0.873 0.990 0.790 0.981 0.993 0.989 0.999 0.954 0.981 0.785 0.885 0.959 
  378   379   380   381   382   383   384   385   386   387   388   389   390 
0.999 0.999 0.726 0.882 0.990 0.986 0.841 0.985 0.649 0.998 0.998 0.630 0.496 
  391   392   393   394   395   396   397   398   399   400   401   402   403 
0.855 0.873 0.973 0.941 0.842 0.834 0.963 0.516 0.887 0.771 0.668 0.528 0.518 
  404   405   406   407   408   409   410   411   412   413   414   415   416 
0.999 0.994 0.491 0.954 0.472 0.993 0.992 0.462 0.995 0.997 0.900 0.977 0.898 
  417   418   419   420   421   422   423   424   425   426   427   428   429 
0.831 0.993 0.987 0.998 0.972 0.523 0.990 0.998 0.987 0.964 0.961 0.864 0.959 
  430   431   432   433   434   435   436   437   438   439   440   441   442 
0.998 0.783 0.996 0.984 0.998 0.806 0.877 0.998 0.994 0.954 0.996 0.999 0.811 
  443   444   445   446   447   448   449   450   451   452   453   454   455 
0.861 0.999 0.899 0.961 0.995 0.929 0.997 0.987 0.618 0.980 0.995 0.750 0.997 
  456   457   458   459   460   461   462   463   464   465   466   467   468 
0.930 0.835 0.999 0.992 0.926 0.768 0.876 0.807 0.993 0.905 0.981 0.752 0.805 
  469   470   471   472   473   474   475   476   477   478   479   480   481 
0.999 0.632 0.931 0.995 0.840 0.956 0.516 0.985 0.949 0.877 0.726 0.514 0.842 
  482   483   484   485   486   487   488   489   490   491   492   493   494 
0.949 0.859 0.999 0.510 0.544 0.984 0.660 0.965 0.569 0.981 0.857 0.699 0.984 
  495   496   497   498   499   500 
0.489 0.910 0.999 0.895 0.998 0.580 
> round( residuals( ss ), 3 )
  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 
 NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 
Warning message:
In Ops.factor(oResponse, fitted(object)) : - not meaningful for factors
> all.equal( residuals( ss ), residuals( ss, part = "outcome" ) )
[1] TRUE
Warning messages:
1: In Ops.factor(oResponse, fitted(object)) : - not meaningful for factors
2: In Ops.factor(oResponse, fitted(object)) : - not meaningful for factors
> round( residuals( ss, part = "selection" ), 3 )
     1      2      3      4      5      6      7      8      9     10     11 
 0.510  0.073  0.043  0.075  0.437  0.852 -1.278  0.264  1.107  0.323  0.390 
    12     13     14     15     16     17     18     19     20     21     22 
 1.156  0.253  0.109  0.249 -1.153  0.497  1.028  1.211  0.085  0.606  0.661 
    23     24     25     26     27     28     29     30     31     32     33 
 0.093  0.470  1.243  1.147  0.144  1.100  1.107 -2.753  0.471  0.446 -1.273 
    34     35     36     37     38     39     40     41     42     43     44 
-1.760  0.107  1.061  0.692  0.965  0.192  0.041  0.813  0.533 -1.363  0.113 
    45     46     47     48     49     50     51     52     53     54     55 
 1.117  0.113  0.991  0.116  0.550  0.233  0.057 -1.332  0.154 -1.455  0.719 
    56     57     58     59     60     61     62     63     64     65     66 
 0.118  0.253 -1.308 -1.460  0.164  0.705 -1.811  0.285  0.575  0.037  0.807 
    67     68     69     70     71     72     73     74     75     76     77 
 0.256  0.477  0.309  0.099  1.242  0.183  0.070  0.479  0.065  0.100  1.147 
    78     79     80     81     82     83     84     85     86     87     88 
 0.048  0.357  0.359  0.833  0.178  0.301  0.239  0.637  0.907  0.083 -1.506 
    89     90     91     92     93     94     95     96     97     98     99 
 0.261  0.042  0.316  0.099  0.202  0.120  1.187  0.289  0.999  0.175  0.058 
   100    101    102    103    104    105    106    107    108    109    110 
 0.287  0.272  0.336  0.196  0.259 -1.272  0.924  0.039  0.038 -1.806  0.110 
   111    112    113    114    115    116    117    118    119    120    121 
 0.174  0.489  0.495  0.067  0.094 -1.729  0.873  0.046  0.087  0.573  0.085 
   122    123    124    125    126    127    128    129    130    131    132 
 1.243 -1.600  0.058  1.155  0.073  0.198  0.798  1.245 -1.271  0.122 -1.165 
   133    134    135    136    137    138    139    140    141    142    143 
 0.117 -2.070  0.112  1.190  0.542  0.821  0.044  0.282  0.432  0.102  0.904 
   144    145    146    147    148    149    150    151    152    153    154 
-1.578  0.570  0.419 -1.327  0.247 -1.321  0.046  0.266  0.898  0.334 -1.213 
   155    156    157    158    159    160    161    162    163    164    165 
 0.660 -1.163  0.471  0.157  0.436  0.386  0.521  0.115  0.045  0.428 -1.759 
   166    167    168    169    170    171    172    173    174    175    176 
 0.201  0.129  0.258  0.541  0.345  0.948  0.710  0.888  0.064  0.285  0.580 
   177    178    179    180    181    182    183    184    185    186    187 
 0.680  0.133  0.285  0.282  0.217  0.045  0.121  0.125  0.931 -2.000  0.058 
   188    189    190    191    192    193    194    195    196    197    198 
 0.360  1.203  0.420 -1.151 -1.131  0.073  0.047  0.065  0.079 -1.375 -1.618 
   199    200    201    202    203    204    205    206    207    208    209 
 0.172  0.204 -1.299  0.252  0.049  0.981  0.327  0.053  0.431  0.488  0.543 
   210    211    212    213    214    215    216    217    218    219    220 
 0.274 -1.334  0.197  0.104  0.916  0.357 -1.412  0.173  0.380  0.820  0.152 
   221    222    223    224    225    226    227    228    229    230    231 
 0.595  0.321  0.254  0.126 -1.180  0.219 -1.489  0.159  0.714  1.085  0.155 
   232    233    234    235    236    237    238    239    240    241    242 
 0.131 -1.489  0.807  0.964  0.616  0.195  0.409  0.794 -1.202  0.084  0.406 
   243    244    245    246    247    248    249    250    251    252    253 
 1.164  0.600 -1.953  0.766  0.818  0.669  0.651  0.065  0.124  1.033  0.127 
   254    255    256    257    258    259    260    261    262    263    264 
 0.469  0.795 -1.106  1.197  0.740  0.796  0.637  0.060  0.051  0.366  0.899 
   265    266    267    268    269    270    271    272    273    274    275 
-1.535 -1.542  0.352 -1.275 -3.214  0.276  0.043  0.071  0.160  1.009  0.914 
   276    277    278    279    280    281    282    283    284    285    286 
 0.592  0.038  0.058  0.315  0.746  0.056  0.130  0.062  0.202 -1.137 -1.310 
   287    288    289    290    291    292    293    294    295    296    297 
 0.048  0.067  0.412  0.111  0.834  0.629  0.321  0.869  0.318 -1.947  0.344 
   298    299    300    301    302    303    304    305    306    307    308 
 0.108  0.432  0.675  0.052  1.196  0.082  0.429  0.218  0.115  0.685  0.992 
   309    310    311    312    313    314    315    316    317    318    319 
 0.065  0.482  0.129 -1.345  0.587  0.302  0.630  0.424  0.484  0.372  0.435 
   320    321    322    323    324    325    326    327    328    329    330 
 0.710  0.201  0.761  0.098  0.100  0.258 -2.481  1.160  0.453  0.211  1.265 
   331    332    333    334    335    336    337    338    339    340    341 
 0.075  1.040 -1.309  0.116  1.198  0.580  0.111 -1.351  0.567  0.600  0.167 
   342    343    344    345    346    347    348    349    350    351    352 
-1.962  0.481 -1.841  0.045  0.172  0.068  0.052  0.135  0.086  0.105  0.065 
   353    354    355    356    357    358    359    360    361    362    363 
 0.090  0.535  0.073  0.104 -1.507  0.909  0.049  0.427 -1.269  0.039 -1.126 
   364    365    366    367    368    369    370    371    372    373    374 
 0.362 -2.159  0.520  0.140  0.687  0.198  0.115  0.149  0.040  0.306  0.195 
   375    376    377    378    379    380    381    382    383    384    385 
 0.695  0.495  0.289  0.040  0.044  0.801  0.501  0.142  0.171 -1.917  0.171 
   386    387    388    389    390    391    392    393    394    395    396 
-1.448  0.066  0.067 -1.410  1.184  0.560 -2.030  0.234  0.348  0.587  0.602 
   397    398    399    400    401    402    403    404    405    406    407 
 0.274  1.150  0.489 -1.717 -1.485  1.130 -1.209  0.042  0.107  1.194  0.306 
   408    409    410    411    412    413    414    415    416    417    418 
 1.226  0.117  0.127 -1.114  0.096  0.079 -2.148  0.216  0.464  0.609  0.123 
   419    420    421    422    423    424    425    426    427    428    429 
 0.161  0.057  0.238  1.139  0.140  0.062  0.163  0.269  0.282  0.540  0.291 
   430    431    432    433    434    435    436    437    438    439    440 
 0.064  0.700  0.092  0.182  0.063  0.657  0.512  0.061  0.114  0.307  0.092 
   441    442    443    444    445    446    447    448    449    450    451 
 0.055  0.647  0.547  0.045 -2.140  0.283  0.104  0.383  0.073  0.163 -1.387 
   452    453    454    455    456    457    458    459    460    461    462 
 0.202  0.105 -1.665  0.076  0.382  0.601  0.037  0.126  0.392 -1.709  0.515 
   463    464    465    466    467    468    469    470    471    472    473 
 0.655  0.121  0.448  0.198  0.754  0.659  0.053 -1.414  0.378  0.103 -1.913 
   474    475    476    477    478    479    480    481    482    483    484 
 0.301  1.151  0.175  0.325  0.512  0.801 -1.201  0.586  0.323  0.552  0.038 
   485    486    487    488    489    490    491    492    493    494    495 
 1.160  1.103  0.182  0.911  0.267 -1.298  0.198  0.556  0.846  0.178 -1.159 
   496    497    498    499    500 
 0.435  0.042  0.472  0.057  1.044 
> model.matrix( ss )
    (Intercept)      xo
1             1 0.32067
2             1 0.52602
3             1 0.07334
4             1 0.84974
5             1 0.42306
6             1 0.98810
7            NA      NA
8             1 0.90569
9             1 0.01851
10            1 0.19214
11            1 0.38431
12            1 0.30744
13            1 0.52829
14            1 0.72823
15            1 0.95356
16           NA      NA
17            1 0.13202
18            1 0.60847
19            1 0.99187
20            1 0.09471
21            1 0.89528
22            1 0.33486
23            1 0.47218
24            1 0.17712
25            1 0.35166
26            1 0.17656
27            1 0.65305
28            1 0.23610
29            1 0.89716
30           NA      NA
31            1 0.92380
32            1 0.49774
33           NA      NA
34           NA      NA
35            1 0.21483
36            1 0.30531
37            1 0.86350
38            1 0.19572
39            1 0.85095
40            1 0.94697
41            1 0.27456
42            1 0.83986
43           NA      NA
44            1 0.75705
45            1 0.44613
46            1 0.95543
47            1 0.63533
48            1 0.16410
49            1 0.40697
50            1 0.97976
51            1 0.14694
52           NA      NA
53            1 0.19670
54           NA      NA
55            1 0.79254
56            1 0.73380
57            1 0.75172
58           NA      NA
59           NA      NA
60            1 0.45588
61            1 0.25400
62           NA      NA
63            1 0.43530
64            1 0.77113
65            1 0.12916
66            1 0.98392
67            1 0.81418
68            1 0.98168
69            1 0.03634
70            1 0.56741
71            1 0.40172
72            1 0.45705
73            1 0.40332
74            1 0.57221
75            1 0.26025
76            1 0.87409
77            1 0.99309
78            1 0.93020
79            1 0.98653
80            1 0.65834
81            1 0.30239
82            1 0.86742
83            1 0.67126
84            1 0.48920
85            1 0.62858
86            1 0.69066
87            1 0.67229
88           NA      NA
89            1 0.68774
90            1 0.99986
91            1 0.25054
92            1 0.47847
93            1 0.01510
94            1 0.25202
95            1 0.32666
96            1 0.67213
97            1 0.74844
98            1 0.34244
99            1 0.28026
100           1 0.73738
101           1 0.62697
102           1 0.63354
103           1 0.21598
104           1 0.90594
105          NA      NA
106           1 0.16858
107           1 0.68320
108           1 0.11399
109          NA      NA
110           1 0.94151
111           1 0.14895
112           1 0.58912
113           1 0.87828
114           1 0.17816
115           1 0.47083
116          NA      NA
117           1 0.09938
118           1 0.29035
119           1 0.24548
120           1 0.72256
121           1 0.53627
122           1 0.29900
123          NA      NA
124           1 0.91018
125           1 0.03297
126           1 0.59632
127           1 0.15091
128           1 0.57765
129           1 0.45448
130          NA      NA
131           1 0.52621
132          NA      NA
133           1 0.92890
134          NA      NA
135           1 0.24476
136           1 0.87626
137           1 0.18266
138           1 0.09032
139           1 0.23916
140           1 0.86143
141           1 0.83256
142           1 0.36291
143           1 0.18791
144          NA      NA
145           1 0.44079
146           1 0.47671
147          NA      NA
148           1 0.08918
149          NA      NA
150           1 0.63567
151           1 0.19854
152           1 0.26161
153           1 0.31876
154          NA      NA
155           1 0.05209
156          NA      NA
157           1 0.57226
158           1 0.03691
159           1 0.01809
160           1 0.19655
161           1 0.34098
162           1 0.28365
163           1 0.18633
164           1 0.20442
165          NA      NA
166           1 0.40610
167           1 0.86717
168           1 0.27581
169           1 0.75255
170           1 0.34063
171           1 0.96010
172           1 0.62854
173           1 0.84531
174           1 0.12228
175           1 0.55571
176           1 0.77643
177           1 0.25938
178           1 0.49898
179           1 0.58442
180           1 0.48475
181           1 0.60854
182           1 0.02086
183           1 0.38280
184           1 0.38162
185           1 0.06741
186          NA      NA
187           1 0.76474
188           1 0.80158
189           1 0.20403
190           1 0.91786
191          NA      NA
192          NA      NA
193           1 0.51343
194           1 0.13278
195           1 0.33798
196           1 0.21300
197          NA      NA
198          NA      NA
199           1 0.33625
200           1 0.14465
201          NA      NA
202           1 0.05432
203           1 0.61388
204           1 0.78563
205           1 0.57945
206           1 0.52717
207           1 0.41051
208           1 0.73052
209           1 0.88360
210           1 0.11448
211          NA      NA
212           1 0.36676
213           1 0.25395
214           1 0.12075
215           1 0.58297
216          NA      NA
217           1 0.57975
218           1 0.59065
219           1 0.97719
220           1 0.24613
221           1 0.47724
222           1 0.16025
223           1 0.78917
224           1 0.09575
225          NA      NA
226           1 0.07575
227          NA      NA
228           1 0.19411
229           1 0.71458
230           1 0.95579
231           1 0.74186
232           1 0.25603
233          NA      NA
234           1 0.13282
235           1 0.18019
236           1 0.07994
237           1 0.27118
238           1 0.44013
239           1 0.71146
240          NA      NA
241           1 0.15047
242           1 0.59951
243           1 0.89062
244           1 0.38870
245          NA      NA
246           1 0.80721
247           1 0.11808
248           1 0.76427
249           1 0.12625
250           1 0.85453
251           1 0.53677
252           1 0.59619
253           1 0.86305
254           1 0.86744
255           1 0.05909
256          NA      NA
257           1 0.34248
258           1 0.82457
259           1 0.52929
260           1 0.43724
261           1 0.05643
262           1 0.24777
263           1 0.39809
264           1 0.11121
265          NA      NA
266          NA      NA
267           1 0.85053
268          NA      NA
269          NA      NA
270           1 0.71772
271           1 0.09516
272           1 0.47922
273           1 0.12714
274           1 0.26424
275           1 0.22704
276           1 0.18450
277           1 0.01726
278           1 0.28163
279           1 0.12104
280           1 0.40643
281           1 0.78917
282           1 0.10587
283           1 0.34159
284           1 0.33359
285          NA      NA
286          NA      NA
287           1 0.78717
288           1 0.79759
289           1 0.22391
290           1 0.46616
291           1 0.32234
292           1 0.54453
293           1 0.97157
294           1 0.14930
295           1 0.36368
296          NA      NA
297           1 0.58661
298           1 0.53979
299           1 0.87389
300           1 0.76943
301           1 0.45084
302           1 0.93905
303           1 0.55859
304           1 0.57100
305           1 0.10731
306           1 0.38876
307           1 0.97748
308           1 0.22145
309           1 0.22006
310           1 0.28217
311           1 0.11922
312          NA      NA
313           1 0.28445
314           1 0.64428
315           1 0.96036
316           1 0.07109
317           1 0.03792
318           1 0.04046
319           1 0.41692
320           1 0.25226
321           1 0.10859
322           1 0.86088
323           1 0.15139
324           1 0.54888
325           1 0.51072
326          NA      NA
327           1 0.68298
328           1 0.74844
329           1 0.95252
330           1 0.36261
331           1 0.34629
332           1 0.45423
333          NA      NA
334           1 0.52909
335           1 0.00282
336           1 0.86187
337           1 0.55689
338          NA      NA
339           1 0.95033
340           1 0.90017
341           1 0.69395
342          NA      NA
343           1 0.46319
344          NA      NA
345           1 0.43910
346           1 0.96992
347           1 0.27955
348           1 0.97947
349           1 0.68293
350           1 0.90801
351           1 0.66998
352           1 0.41306
353           1 0.93139
354           1 0.14916
355           1 0.01702
356           1 0.57650
357          NA      NA
358           1 0.28937
359           1 0.27640
360           1 0.60834
361          NA      NA
362           1 0.29098
363          NA      NA
364           1 0.82154
365          NA      NA
366           1 0.98572
367           1 0.61545
368           1 0.83248
369           1 0.23662
370           1 0.77090
371           1 0.05877
372           1 0.51490
373           1 0.86101
374           1 0.98540
375           1 0.39676
376           1 0.46828
377           1 0.42066
378           1 0.91563
379           1 0.48231
380           1 0.94241
381           1 0.12418
382           1 0.56371
383           1 0.73124
384          NA      NA
385           1 0.62836
386          NA      NA
387           1 0.08576
388           1 0.67620
389          NA      NA
390           1 0.09610
391           1 0.39890
392          NA      NA
393           1 0.79457
394           1 0.62454
395           1 0.40501
396           1 0.00259
397           1 0.60858
398           1 0.59234
399           1 0.91016
400          NA      NA
401          NA      NA
402           1 0.25509
403          NA      NA
404           1 0.52525
405           1 0.45407
406           1 0.52172
407           1 0.08570
408           1 0.59755
409           1 0.68352
410           1 0.54936
411          NA      NA
412           1 0.29624
413           1 0.28450
414          NA      NA
415           1 0.16767
416           1 0.91739
417           1 0.99091
418           1 0.74767
419           1 0.49254
420           1 0.93281
421           1 0.61704
422           1 0.39170
423           1 0.19638
424           1 0.72008
425           1 0.93417
426           1 0.69264
427           1 0.88858
428           1 0.22867
429           1 0.54657
430           1 0.20700
431           1 0.53705
432           1 0.96928
433           1 0.80323
434           1 0.20405
435           1 0.40065
436           1 0.82912
437           1 0.44113
438           1 0.39505
439           1 0.03089
440           1 0.13587
441           1 0.67718
442           1 0.06249
443           1 0.99754
444           1 0.11385
445          NA      NA
446           1 0.02559
447           1 0.10831
448           1 0.45408
449           1 0.80019
450           1 0.83482
451          NA      NA
452           1 0.28282
453           1 0.64290
454          NA      NA
455           1 0.53711
456           1 0.43145
457           1 0.72412
458           1 0.17412
459           1 0.32032
460           1 0.38798
461          NA      NA
462           1 0.27792
463           1 0.71545
464           1 0.32855
465           1 0.60146
466           1 0.52392
467           1 0.86055
468           1 0.53277
469           1 0.11142
470          NA      NA
471           1 0.06294
472           1 0.70529
473          NA      NA
474           1 0.78132
475           1 0.78414
476           1 0.55258
477           1 0.85511
478           1 0.20987
479           1 0.23006
480          NA      NA
481           1 0.44478
482           1 0.66007
483           1 0.07782
484           1 0.34325
485           1 0.80745
486           1 0.34898
487           1 0.05208
488           1 0.86328
489           1 0.55692
490          NA      NA
491           1 0.76130
492           1 0.54554
493           1 0.47788
494           1 0.34743
495          NA      NA
496           1 0.95077
497           1 0.26138
498           1 0.10681
499           1 0.25376
500           1 0.18090
attr(,"assign")
[1] 0 1
> all.equal( model.matrix( ss ), model.matrix( ss, part = "outcome" ) )
[1] TRUE
> model.matrix( ss, part = "selection" )
    (Intercept)       xs
1             1 0.387113
2             1 0.871805
3             1 0.967197
4             1 0.866916
5             1 0.437715
6             1 0.191938
7             1 0.082294
8             1 0.583452
9             1 0.070361
10            1 0.527663
11            1 0.472288
12            1 0.048191
13            1 0.594541
14            1 0.791271
15            1 0.598869
16            1 0.027916
17            1 0.395727
18            1 0.106244
19            1 0.024111
20            1 0.840898
21            1 0.326400
22            1 0.294298
23            1 0.822898
24            1 0.414347
25            1 0.010171
26            1 0.052408
27            1 0.730370
28            1 0.073395
29            1 0.070357
30            1 0.637172
31            1 0.413306
32            1 0.430718
33            1 0.080246
34            1 0.277117
35            1 0.794071
36            1 0.091097
37            1 0.276682
38            1 0.136255
39            1 0.663476
40            1 0.979130
41            1 0.211811
42            1 0.372027
43            1 0.118365
44            1 0.782724
45            1 0.065578
46            1 0.783775
47            1 0.123852
48            1 0.777342
49            1 0.361349
50            1 0.615930
51            1 0.917043
52            1 0.105220
53            1 0.715546
54            1 0.156367
55            1 0.261576
56            1 0.773791
57            1 0.594759
58            1 0.095377
59            1 0.158468
60            1 0.701091
61            1 0.269183
62            1 0.296830
63            1 0.562665
64            1 0.345177
65            1 0.996363
66            1 0.214890
67            1 0.590747
68            1 0.408991
69            1 0.539929
70            1 0.809807
71            1 0.010653
72            1 0.674990
73            1 0.880182
74            1 0.408089
75            1 0.894052
76            1 0.807851
77            1 0.052268
78            1 0.951139
79            1 0.498940
80            1 0.497459
81            1 0.201222
82            1 0.680997
83            1 0.547304
84            1 0.609578
85            1 0.308183
86            1 0.164061
87            1 0.847400
88            1 0.177071
89            1 0.586211
90            1 0.972557
91            1 0.534099
92            1 0.810916
93            1 0.650907
94            1 0.770981
95            1 0.034558
96            1 0.558660
97            1 0.120087
98            1 0.686300
99            1 0.914126
100           1 0.560350
101           1 0.575576
102           1 0.516750
103           1 0.658049
104           1 0.588397
105           1 0.079925
106           1 0.156136
107           1 0.985081
108           1 0.991830
109           1 0.294756
110           1 0.788712
111           1 0.686831
112           1 0.400827
113           1 0.396841
114           1 0.887414
115           1 0.822078
116           1 0.265245
117           1 0.181171
118           1 0.956474
119           1 0.837400
120           1 0.346653
121           1 0.842086
122           1 0.010130
123           1 0.214542
124           1 0.913735
125           1 0.048655
126           1 0.871231
127           1 0.655788
128           1 0.219372
129           1 0.009437
130           1 0.079214
131           1 0.767451
132           1 0.033288
133           1 0.774952
134           1 0.393960
135           1 0.784817
136           1 0.033332
137           1 0.366152
138           1 0.207688
139           1 0.965913
140           1 0.564966
141           1 0.441116
142           1 0.803868
143           1 0.165633
144           1 0.206119
145           1 0.348587
146           1 0.450815
147           1 0.103177
148           1 0.600618
149           1 0.100578
150           1 0.957186
151           1 0.580974
152           1 0.168894
153           1 0.518571
154           1 0.054323
155           1 0.295023
156           1 0.032474
157           1 0.413448
158           1 0.710805
159           1 0.438294
160           1 0.476030
161           1 0.379705
162           1 0.779968
163           1 0.961577
164           1 0.443998
165           1 0.276577
166           1 0.652875
167           1 0.754724
168           1 0.588706
169           1 0.366898
170           1 0.509391
171           1 0.144229
172           1 0.266655
173           1 0.173821
174           1 0.896527
175           1 0.562343
176           1 0.342454
177           1 0.283730
178           1 0.747538
179           1 0.562850
180           1 0.565386
181           1 0.633332
182           1 0.962587
183           1 0.768941
184           1 0.761524
185           1 0.152722
186           1 0.367798
187           1 0.913616
188           1 0.496350
189           1 0.027735
190           1 0.450139
191           1 0.027188
192           1 0.018271
193           1 0.871962
194           1 0.955189
195           1 0.892738
196           1 0.856309
197           1 0.123533
198           1 0.221655
199           1 0.690272
200           1 0.648772
201           1 0.091445
202           1 0.595570
203           1 0.944378
204           1 0.128658
205           1 0.524702
206           1 0.931876
207           1 0.441982
208           1 0.401972
209           1 0.365523
210           1 0.572830
211           1 0.106167
212           1 0.657214
213           1 0.800479
214           1 0.160081
215           1 0.499150
216           1 0.138866
217           1 0.688598
218           1 0.480642
219           1 0.207813
220           1 0.717721
221           1 0.333421
222           1 0.529427
223           1 0.593499
224           1 0.759889
225           1 0.039696
226           1 0.631180
227           1 0.170350
228           1 0.707257
229           1 0.264722
230           1 0.080322
231           1 0.713207
232           1 0.750814
233           1 0.170226
234           1 0.214822
235           1 0.136855
236           1 0.320705
237           1 0.659653
238           1 0.458247
239           1 0.221784
240           1 0.049692
241           1 0.842967
242           1 0.460356
243           1 0.044669
244           1 0.330020
245           1 0.350482
246           1 0.236221
247           1 0.209323
248           1 0.289585
249           1 0.299830
250           1 0.893524
251           1 0.762661
252           1 0.104029
253           1 0.758670
254           1 0.414734
255           1 0.221036
256           1 0.006719
257           1 0.030251
258           1 0.250102
259           1 0.220495
260           1 0.308081
261           1 0.908893
262           1 0.938503
263           1 0.491905
264           1 0.168020
265           1 0.188895
266           1 0.191555
267           1 0.503482
268           1 0.081112
269           1 0.794697
270           1 0.571347
271           1 0.967688
272           1 0.875769
273           1 0.705853
274           1 0.115355
275           1 0.160992
276           1 0.335154
277           1 0.988609
278           1 0.914079
279           1 0.535097
280           1 0.246894
281           1 0.922347
282           1 0.753490
283           1 0.903507
284           1 0.651548
285           1 0.020779
286           1 0.095879
287           1 0.949587
288           1 0.888177
289           1 0.456234
290           1 0.786744
291           1 0.201027
292           1 0.312703
293           1 0.529762
294           1 0.182999
295           1 0.532406
296           1 0.348114
297           1 0.510248
298           1 0.792196
299           1 0.441211
300           1 0.286292
301           1 0.933560
302           1 0.030451
303           1 0.848986
304           1 0.443183
305           1 0.632075
306           1 0.779914
307           1 0.280474
308           1 0.123204
309           1 0.893433
310           1 0.405973
311           1 0.755108
312           1 0.110817
313           1 0.338309
314           1 0.546493
315           1 0.312132
316           1 0.447295
317           1 0.404281
318           1 0.486564
319           1 0.439168
320           1 0.266864
321           1 0.651878
322           1 0.239049
323           1 0.813316
324           1 0.808245
325           1 0.589045
326           1 0.541867
327           1 0.046548
328           1 0.425856
329           1 0.640640
330           1 0.000656
331           1 0.866388
332           1 0.101045
333           1 0.095740
334           1 0.777240
335           1 0.029511
336           1 0.342072
337           1 0.786316
338           1 0.113463
339           1 0.350253
340           1 0.330214
341           1 0.696677
342           1 0.353614
343           1 0.406179
344           1 0.308030
345           1 0.959203
346           1 0.690269
347           1 0.885740
348           1 0.935855
349           1 0.744080
350           1 0.838446
351           1 0.799357
352           1 0.893640
353           1 0.831137
354           1 0.370750
355           1 0.871980
356           1 0.800762
357           1 0.177285
358           1 0.163278
359           1 0.944489
360           1 0.444638
361           1 0.078393
362           1 0.985572
363           1 0.015838
364           1 0.494883
365           1 0.426626
366           1 0.380065
367           1 0.736944
368           1 0.279538
369           1 0.656284
370           1 0.779726
371           1 0.722053
372           1 0.982406
373           1 0.543323
374           1 0.659888
375           1 0.275039
376           1 0.396944
377           1 0.559185
378           1 0.980950
379           1 0.964967
380           1 0.218040
381           1 0.393171
382           1 0.734231
383           1 0.691589
384           1 0.336752
385           1 0.690482
386           1 0.153450
387           1 0.890399
388           1 0.888446
389           1 0.138003
390           1 0.035949
391           1 0.355017
392           1 0.378946
393           1 0.614777
394           1 0.506189
395           1 0.337955
396           1 0.329130
397           1 0.573297
398           1 0.050710
399           1 0.400963
400           1 0.260631
401           1 0.168700
402           1 0.059731
403           1 0.052582
404           1 0.972770
405           1 0.794608
406           1 0.031668
407           1 0.543285
408           1 0.017597
409           1 0.776670
410           1 0.757301
411           1 0.010548
412           1 0.817914
413           1 0.856346
414           1 0.422310
415           1 0.634296
416           1 0.418075
417           1 0.324664
418           1 0.765787
419           1 0.704656
420           1 0.919051
421           1 0.610137
422           1 0.055855
423           1 0.736152
424           1 0.903968
425           1 0.701815
426           1 0.577700
427           1 0.565839
428           1 0.367176
429           1 0.556922
430           1 0.897090
431           1 0.272231
432           1 0.826333
433           1 0.675996
434           1 0.899367
435           1 0.296660
436           1 0.385464
437           1 0.905350
438           1 0.781176
439           1 0.542551
440           1 0.826008
441           1 0.925736
442           1 0.302508
443           1 0.363235
444           1 0.960561
445           1 0.419576
446           1 0.564240
447           1 0.800476
448           1 0.478195
449           1 0.871664
450           1 0.701587
451           1 0.128367
452           1 0.651634
453           1 0.799508
454           1 0.240165
455           1 0.863995
456           1 0.478795
457           1 0.329742
458           1 0.995426
459           1 0.759542
460           1 0.471065
461           1 0.257584
462           1 0.383511
463           1 0.297954
464           1 0.768626
465           1 0.429740
466           1 0.655967
467           1 0.242657
468           1 0.295185
469           1 0.929919
470           1 0.139466
471           1 0.482361
472           1 0.801726
473           1 0.335298
474           1 0.547301
475           1 0.050614
476           1 0.685185
477           1 0.526017
478           1 0.385401
479           1 0.218078
480           1 0.049256
481           1 0.338604
482           1 0.527543
483           1 0.360100
484           1 0.991589
485           1 0.046358
486           1 0.071902
487           1 0.676367
488           1 0.162360
489           1 0.580374
490           1 0.090766
491           1 0.656099
492           1 0.357507
493           1 0.194491
494           1 0.681934
495           1 0.030555
496           1 0.439074
497           1 0.973690
498           1 0.412593
499           1 0.919770
500           1 0.098764
attr(,"assign")
[1] 0 1
> model.frame( ss )
       ys       xs yo       xo
1    TRUE 0.387113  0 0.320667
2    TRUE 0.871805  1 0.526019
3    TRUE 0.967197  0 0.073335
4    TRUE 0.866916  1 0.849742
5    TRUE 0.437715  1 0.423058
6    TRUE 0.191938  1 0.988096
7   FALSE 0.082294  0 0.478874
8    TRUE 0.583452  1 0.905694
9    TRUE 0.070361  0 0.018506
10   TRUE 0.527663  0 0.192144
11   TRUE 0.472288  1 0.384307
12   TRUE 0.048191  1 0.307436
13   TRUE 0.594541  1 0.528291
14   TRUE 0.791271  1 0.728226
15   TRUE 0.598869  1 0.953557
16  FALSE 0.027916  0 0.495994
17   TRUE 0.395727  0 0.132017
18   TRUE 0.106244  1 0.608469
19   TRUE 0.024111  1 0.991869
20   TRUE 0.840898  0 0.094710
21   TRUE 0.326400  1 0.895283
22   TRUE 0.294298  0 0.334861
23   TRUE 0.822898  0 0.472178
24   TRUE 0.414347  0 0.177118
25   TRUE 0.010171  0 0.351655
26   TRUE 0.052408  0 0.176559
27   TRUE 0.730370  1 0.653046
28   TRUE 0.073395  0 0.236101
29   TRUE 0.070357  1 0.897164
30  FALSE 0.637172  0 0.339148
31   TRUE 0.413306  1 0.923796
32   TRUE 0.430718  0 0.497739
33  FALSE 0.080246  0 0.569841
34  FALSE 0.277117  0 0.736780
35   TRUE 0.794071  0 0.214828
36   TRUE 0.091097  0 0.305313
37   TRUE 0.276682  1 0.863500
38   TRUE 0.136255  1 0.195720
39   TRUE 0.663476  1 0.850946
40   TRUE 0.979130  0 0.946971
41   TRUE 0.211811  1 0.274561
42   TRUE 0.372027  1 0.839856
43  FALSE 0.118365  0 0.143856
44   TRUE 0.782724  1 0.757050
45   TRUE 0.065578  1 0.446127
46   TRUE 0.783775  1 0.955432
47   TRUE 0.123852  0 0.635325
48   TRUE 0.777342  0 0.164101
49   TRUE 0.361349  1 0.406969
50   TRUE 0.615930  1 0.979760
51   TRUE 0.917043  0 0.146939
52  FALSE 0.105220  0 0.889985
53   TRUE 0.715546  0 0.196703
54  FALSE 0.156367  0 0.620587
55   TRUE 0.261576  1 0.792541
56   TRUE 0.773791  0 0.733796
57   TRUE 0.594759  1 0.751720
58  FALSE 0.095377  0 0.524456
59  FALSE 0.158468  0 0.629952
60   TRUE 0.701091  1 0.455877
61   TRUE 0.269183  0 0.254005
62  FALSE 0.296830  0 0.057486
63   TRUE 0.562665  1 0.435303
64   TRUE 0.345177  1 0.771135
65   TRUE 0.996363  0 0.129155
66   TRUE 0.214890  1 0.983916
67   TRUE 0.590747  1 0.814177
68   TRUE 0.408991  1 0.981681
69   TRUE 0.539929  0 0.036340
70   TRUE 0.809807  0 0.567414
71   TRUE 0.010653  1 0.401718
72   TRUE 0.674990  0 0.457046
73   TRUE 0.880182  0 0.403324
74   TRUE 0.408089  0 0.572206
75   TRUE 0.894052  0 0.260251
76   TRUE 0.807851  1 0.874088
77   TRUE 0.052268  1 0.993087
78   TRUE 0.951139  1 0.930202
79   TRUE 0.498940  1 0.986528
80   TRUE 0.497459  0 0.658343
81   TRUE 0.201222  0 0.302385
82   TRUE 0.680997  1 0.867420
83   TRUE 0.547304  1 0.671260
84   TRUE 0.609578  1 0.489200
85   TRUE 0.308183  0 0.628584
86   TRUE 0.164061  1 0.690660
87   TRUE 0.847400  0 0.672291
88  FALSE 0.177071  0 0.882118
89   TRUE 0.586211  0 0.687738
90   TRUE 0.972557  0 0.999863
91   TRUE 0.534099  0 0.250536
92   TRUE 0.810916  1 0.478467
93   TRUE 0.650907  0 0.015101
94   TRUE 0.770981  0 0.252020
95   TRUE 0.034558  1 0.326658
96   TRUE 0.558660  1 0.672132
97   TRUE 0.120087  1 0.748441
98   TRUE 0.686300  1 0.342436
99   TRUE 0.914126  0 0.280260
100  TRUE 0.560350  0 0.737378
101  TRUE 0.575576  0 0.626969
102  TRUE 0.516750  1 0.633542
103  TRUE 0.658049  1 0.215981
104  TRUE 0.588397  1 0.905941
105 FALSE 0.079925  0 0.586246
106  TRUE 0.156136  0 0.168584
107  TRUE 0.985081  0 0.683198
108  TRUE 0.991830  1 0.113991
109 FALSE 0.294756  0 0.527885
110  TRUE 0.788712  0 0.941510
111  TRUE 0.686831  0 0.148946
112  TRUE 0.400827  1 0.589122
113  TRUE 0.396841  0 0.878276
114  TRUE 0.887414  0 0.178157
115  TRUE 0.822078  1 0.470834
116 FALSE 0.265245  0 0.141462
117  TRUE 0.181171  1 0.099383
118  TRUE 0.956474  0 0.290351
119  TRUE 0.837400  0 0.245476
120  TRUE 0.346653  1 0.722559
121  TRUE 0.842086  0 0.536275
122  TRUE 0.010130  1 0.299001
123 FALSE 0.214542  0 0.199700
124  TRUE 0.913735  1 0.910177
125  TRUE 0.048655  0 0.032971
126  TRUE 0.871231  1 0.596324
127  TRUE 0.655788  1 0.150905
128  TRUE 0.219372  1 0.577648
129  TRUE 0.009437  1 0.454477
130 FALSE 0.079214  0 0.493809
131  TRUE 0.767451  1 0.526207
132 FALSE 0.033288  0 0.383013
133  TRUE 0.774952  1 0.928898
134 FALSE 0.393960  0 0.433418
135  TRUE 0.784817  0 0.244762
136  TRUE 0.033332  1 0.876257
137  TRUE 0.366152  0 0.182660
138  TRUE 0.207688  0 0.090324
139  TRUE 0.965913  1 0.239160
140  TRUE 0.564966  0 0.861430
141  TRUE 0.441116  0 0.832556
142  TRUE 0.803868  1 0.362913
143  TRUE 0.165633  1 0.187907
144 FALSE 0.206119  0 0.768746
145  TRUE 0.348587  0 0.440788
146  TRUE 0.450815  1 0.476709
147 FALSE 0.103177  0 0.980657
148  TRUE 0.600618  0 0.089181
149 FALSE 0.100578  0 0.097385
150  TRUE 0.957186  1 0.635669
151  TRUE 0.580974  1 0.198539
152  TRUE 0.168894  1 0.261605
153  TRUE 0.518571  0 0.318762
154 FALSE 0.054323  0 0.869577
155  TRUE 0.295023  0 0.052089
156 FALSE 0.032474  0 0.203911
157  TRUE 0.413448  1 0.572259
158  TRUE 0.710805  1 0.036906
159  TRUE 0.438294  0 0.018092
160  TRUE 0.476030  0 0.196550
161  TRUE 0.379705  0 0.340977
162  TRUE 0.779968  0 0.283653
163  TRUE 0.961577  1 0.186334
164  TRUE 0.443998  0 0.204422
165 FALSE 0.276577  0 0.231853
166  TRUE 0.652875  1 0.406102
167  TRUE 0.754724  1 0.867170
168  TRUE 0.588706  0 0.275813
169  TRUE 0.366898  1 0.752555
170  TRUE 0.509391  0 0.340631
171  TRUE 0.144229  1 0.960103
172  TRUE 0.266655  1 0.628541
173  TRUE 0.173821  1 0.845313
174  TRUE 0.896527  0 0.122284
175  TRUE 0.562343  0 0.555707
176  TRUE 0.342454  1 0.776426
177  TRUE 0.283730  0 0.259379
178  TRUE 0.747538  1 0.498980
179  TRUE 0.562850  1 0.584417
180  TRUE 0.565386  0 0.484753
181  TRUE 0.633332  1 0.608540
182  TRUE 0.962587  1 0.020863
183  TRUE 0.768941  0 0.382800
184  TRUE 0.761524  0 0.381620
185  TRUE 0.152722  0 0.067407
186 FALSE 0.367798  0 0.289226
187  TRUE 0.913616  1 0.764743
188  TRUE 0.496350  1 0.801585
189  TRUE 0.027735  0 0.204031
190  TRUE 0.450139  0 0.917860
191 FALSE 0.027188  0 0.102161
192 FALSE 0.018271  0 0.890885
193  TRUE 0.871962  1 0.513431
194  TRUE 0.955189  1 0.132782
195  TRUE 0.892738  1 0.337982
196  TRUE 0.856309  1 0.213000
197 FALSE 0.123533  0 0.828702
198 FALSE 0.221655  0 0.464334
199  TRUE 0.690272  0 0.336253
200  TRUE 0.648772  0 0.144650
201 FALSE 0.091445  0 0.868071
202  TRUE 0.595570  0 0.054323
203  TRUE 0.944378  0 0.613883
204  TRUE 0.128658  1 0.785630
205  TRUE 0.524702  1 0.579447
206  TRUE 0.931876  1 0.527169
207  TRUE 0.441982  0 0.410513
208  TRUE 0.401972  1 0.730523
209  TRUE 0.365523  1 0.883600
210  TRUE 0.572830  1 0.114475
211 FALSE 0.106167  0 0.324148
212  TRUE 0.657214  0 0.366757
213  TRUE 0.800479  0 0.253949
214  TRUE 0.160081  1 0.120749
215  TRUE 0.499150  1 0.582968
216 FALSE 0.138866  0 0.203435
217  TRUE 0.688598  0 0.579752
218  TRUE 0.480642  0 0.590648
219  TRUE 0.207813  1 0.977187
220  TRUE 0.717721  0 0.246125
221  TRUE 0.333421  1 0.477242
222  TRUE 0.529427  1 0.160246
223  TRUE 0.593499  1 0.789169
224  TRUE 0.759889  0 0.095748
225 FALSE 0.039696  0 0.482404
226  TRUE 0.631180  0 0.075754
227 FALSE 0.170350  0 0.937550
228  TRUE 0.707257  0 0.194111
229  TRUE 0.264722  0 0.714579
230  TRUE 0.080322  1 0.955791
231  TRUE 0.713207  1 0.741858
232  TRUE 0.750814  0 0.256030
233 FALSE 0.170226  0 0.612532
234  TRUE 0.214822  0 0.132818
235  TRUE 0.136855  0 0.180193
236  TRUE 0.320705  1 0.079936
237  TRUE 0.659653  0 0.271181
238  TRUE 0.458247  1 0.440128
239  TRUE 0.221784  1 0.711458
240 FALSE 0.049692  0 0.346788
241  TRUE 0.842967  0 0.150468
242  TRUE 0.460356  0 0.599506
243  TRUE 0.044669  1 0.890618
244  TRUE 0.330020  1 0.388701
245 FALSE 0.350482  0 0.098134
246  TRUE 0.236221  1 0.807212
247  TRUE 0.209323  0 0.118084
248  TRUE 0.289585  1 0.764266
249  TRUE 0.299830  0 0.126254
250  TRUE 0.893524  1 0.854532
251  TRUE 0.762661  1 0.536769
252  TRUE 0.104029  0 0.596192
253  TRUE 0.758670  1 0.863054
254  TRUE 0.414734  1 0.867438
255  TRUE 0.221036  0 0.059093
256 FALSE 0.006719  0 0.469490
257  TRUE 0.030251  1 0.342482
258  TRUE 0.250102  1 0.824571
259  TRUE 0.220495  1 0.529290
260  TRUE 0.308081  0 0.437236
261  TRUE 0.908893  0 0.056426
262  TRUE 0.938503  0 0.247775
263  TRUE 0.491905  1 0.398086
264  TRUE 0.168020  1 0.111213
265 FALSE 0.188895  0 0.421623
266 FALSE 0.191555  0 0.239189
267  TRUE 0.503482  1 0.850532
268 FALSE 0.081112  0 0.227025
269 FALSE 0.794697  0 0.747712
270  TRUE 0.571347  0 0.717719
271  TRUE 0.967688  0 0.095165
272  TRUE 0.875769  1 0.479225
273  TRUE 0.705853  0 0.127142
274  TRUE 0.115355  0 0.264237
275  TRUE 0.160992  0 0.227042
276  TRUE 0.335154  0 0.184503
277  TRUE 0.988609  0 0.017256
278  TRUE 0.914079  0 0.281634
279  TRUE 0.535097  0 0.121040
280  TRUE 0.246894  1 0.406426
281  TRUE 0.922347  1 0.789168
282  TRUE 0.753490  1 0.105868
283  TRUE 0.903507  0 0.341595
284  TRUE 0.651548  0 0.333594
285 FALSE 0.020779  0 0.774402
286 FALSE 0.095879  0 0.537167
287  TRUE 0.949587  1 0.787171
288  TRUE 0.888177  0 0.797589
289  TRUE 0.456234  0 0.223906
290  TRUE 0.786744  0 0.466158
291  TRUE 0.201027  0 0.322344
292  TRUE 0.312703  0 0.544530
293  TRUE 0.529762  0 0.971569
294  TRUE 0.182999  0 0.149301
295  TRUE 0.532406  0 0.363683
296 FALSE 0.348114  0 0.787107
297  TRUE 0.510248  0 0.586609
298  TRUE 0.792196  0 0.539789
299  TRUE 0.441211  1 0.873886
300  TRUE 0.286292  1 0.769427
301  TRUE 0.933560  0 0.450837
302  TRUE 0.030451  1 0.939054
303  TRUE 0.848986  1 0.558589
304  TRUE 0.443183  0 0.571000
305  TRUE 0.632075  0 0.107309
306  TRUE 0.779914  0 0.388762
307  TRUE 0.280474  1 0.977478
308  TRUE 0.123204  0 0.221452
309  TRUE 0.893433  0 0.220063
310  TRUE 0.405973  0 0.282171
311  TRUE 0.755108  0 0.119221
312 FALSE 0.110817  0 0.180134
313  TRUE 0.338309  0 0.284447
314  TRUE 0.546493  1 0.644276
315  TRUE 0.312132  1 0.960355
316  TRUE 0.447295  0 0.071095
317  TRUE 0.404281  0 0.037918
318  TRUE 0.486564  0 0.040461
319  TRUE 0.439168  1 0.416925
320  TRUE 0.266864  1 0.252264
321  TRUE 0.651878  0 0.108593
322  TRUE 0.239049  1 0.860879
323  TRUE 0.813316  0 0.151392
324  TRUE 0.808245  1 0.548880
325  TRUE 0.589045  1 0.510725
326 FALSE 0.541867  0 0.310565
327  TRUE 0.046548  1 0.682978
328  TRUE 0.425856  1 0.748440
329  TRUE 0.640640  1 0.952518
330  TRUE 0.000656  0 0.362615
331  TRUE 0.866388  0 0.346294
332  TRUE 0.101045  1 0.454230
333 FALSE 0.095740  0 0.668705
334  TRUE 0.777240  1 0.529092
335  TRUE 0.029511  1 0.002821
336  TRUE 0.342072  0 0.861873
337  TRUE 0.786316  0 0.556886
338 FALSE 0.113463  0 0.901920
339  TRUE 0.350253  1 0.950326
340  TRUE 0.330214  0 0.900168
341  TRUE 0.696677  1 0.693946
342 FALSE 0.353614  0 0.836224
343  TRUE 0.406179  0 0.463189
344 FALSE 0.308030  0 0.629100
345  TRUE 0.959203  1 0.439101
346  TRUE 0.690269  1 0.969921
347  TRUE 0.885740  0 0.279549
348  TRUE 0.935855  1 0.979469
349  TRUE 0.744080  1 0.682925
350  TRUE 0.838446  1 0.908006
351  TRUE 0.799357  1 0.669979
352  TRUE 0.893640  1 0.413065
353  TRUE 0.831137  0 0.931386
354  TRUE 0.370750  0 0.149159
355  TRUE 0.871980  0 0.017022
356  TRUE 0.800762  1 0.576501
357 FALSE 0.177285  0 0.508741
358  TRUE 0.163278  1 0.289367
359  TRUE 0.944489  0 0.276403
360  TRUE 0.444638  1 0.608338
361 FALSE 0.078393  0 0.390518
362  TRUE 0.985572  0 0.290984
363 FALSE 0.015838  0 0.385936
364  TRUE 0.494883  1 0.821543
365 FALSE 0.426626  0 0.232980
366  TRUE 0.380065  1 0.985718
367  TRUE 0.736944  0 0.615446
368  TRUE 0.279538  1 0.832476
369  TRUE 0.656284  0 0.236616
370  TRUE 0.779726  1 0.770901
371  TRUE 0.722053  0 0.058774
372  TRUE 0.982406  0 0.514900
373  TRUE 0.543323  1 0.861013
374  TRUE 0.659888  1 0.985402
375  TRUE 0.275039  1 0.396762
376  TRUE 0.396944  1 0.468284
377  TRUE 0.559185  0 0.420656
378  TRUE 0.980950  1 0.915635
379  TRUE 0.964967  1 0.482314
380  TRUE 0.218040  1 0.942408
381  TRUE 0.393171  1 0.124179
382  TRUE 0.734231  1 0.563708
383  TRUE 0.691589  1 0.731237
384 FALSE 0.336752  0 0.000571
385  TRUE 0.690482  1 0.628356
386 FALSE 0.153450  0 0.370726
387  TRUE 0.890399  0 0.085762
388  TRUE 0.888446  1 0.676197
389 FALSE 0.138003  0 0.609158
390  TRUE 0.035949  1 0.096099
391  TRUE 0.355017  1 0.398905
392 FALSE 0.378946  0 0.098322
393  TRUE 0.614777  1 0.794568
394  TRUE 0.506189  1 0.624536
395  TRUE 0.337955  0 0.405014
396  TRUE 0.329130  1 0.002587
397  TRUE 0.573297  1 0.608580
398  TRUE 0.050710  1 0.592337
399  TRUE 0.400963  1 0.910157
400 FALSE 0.260631  0 0.393951
401 FALSE 0.168700  0 0.964200
402  TRUE 0.059731  0 0.255094
403 FALSE 0.052582  0 0.140772
404  TRUE 0.972770  1 0.525254
405  TRUE 0.794608  0 0.454067
406  TRUE 0.031668  1 0.521722
407  TRUE 0.543285  0 0.085697
408  TRUE 0.017597  1 0.597549
409  TRUE 0.776670  0 0.683516
410  TRUE 0.757301  0 0.549356
411 FALSE 0.010548  0 0.620225
412  TRUE 0.817914  0 0.296236
413  TRUE 0.856346  1 0.284500
414 FALSE 0.422310  0 0.601983
415  TRUE 0.634296  0 0.167672
416  TRUE 0.418075  1 0.917387
417  TRUE 0.324664  1 0.990910
418  TRUE 0.765787  0 0.747668
419  TRUE 0.704656  0 0.492539
420  TRUE 0.919051  1 0.932810
421  TRUE 0.610137  1 0.617039
422  TRUE 0.055855  1 0.391704
423  TRUE 0.736152  1 0.196378
424  TRUE 0.903968  0 0.720079
425  TRUE 0.701815  1 0.934172
426  TRUE 0.577700  1 0.692644
427  TRUE 0.565839  0 0.888581
428  TRUE 0.367176  0 0.228666
429  TRUE 0.556922  0 0.546569
430  TRUE 0.897090  0 0.206999
431  TRUE 0.272231  0 0.537050
432  TRUE 0.826333  1 0.969278
433  TRUE 0.675996  1 0.803234
434  TRUE 0.899367  0 0.204047
435  TRUE 0.296660  1 0.400647
436  TRUE 0.385464  1 0.829125
437  TRUE 0.905350  0 0.441127
438  TRUE 0.781176  1 0.395053
439  TRUE 0.542551  0 0.030889
440  TRUE 0.826008  0 0.135866
441  TRUE 0.925736  1 0.677184
442  TRUE 0.302508  0 0.062488
443  TRUE 0.363235  1 0.997538
444  TRUE 0.960561  0 0.113849
445 FALSE 0.419576  0 0.625333
446  TRUE 0.564240  0 0.025587
447  TRUE 0.800476  0 0.108310
448  TRUE 0.478195  1 0.454083
449  TRUE 0.871664  1 0.800191
450  TRUE 0.701587  0 0.834824
451 FALSE 0.128367  0 0.724688
452  TRUE 0.651634  0 0.282822
453  TRUE 0.799508  1 0.642903
454 FALSE 0.240165  0 0.958786
455  TRUE 0.863995  1 0.537109
456  TRUE 0.478795  1 0.431453
457  TRUE 0.329742  1 0.724119
458  TRUE 0.995426  0 0.174124
459  TRUE 0.759542  1 0.320324
460  TRUE 0.471065  0 0.387978
461 FALSE 0.257584  0 0.054494
462  TRUE 0.383511  0 0.277925
463  TRUE 0.297954  0 0.715454
464  TRUE 0.768626  1 0.328546
465  TRUE 0.429740  1 0.601455
466  TRUE 0.655967  0 0.523921
467  TRUE 0.242657  1 0.860549
468  TRUE 0.295185  1 0.532769
469  TRUE 0.929919  0 0.111422
470 FALSE 0.139466  0 0.775893
471  TRUE 0.482361  1 0.062940
472  TRUE 0.801726  0 0.705285
473 FALSE 0.335298  0 0.049256
474  TRUE 0.547301  0 0.781317
475  TRUE 0.050614  0 0.784139
476  TRUE 0.685185  0 0.552576
477  TRUE 0.526017  1 0.855108
478  TRUE 0.385401  0 0.209872
479  TRUE 0.218078  0 0.230055
480 FALSE 0.049256  0 0.673410
481  TRUE 0.338604  1 0.444783
482  TRUE 0.527543  0 0.660065
483  TRUE 0.360100  1 0.077822
484  TRUE 0.991589  0 0.343248
485  TRUE 0.046358  1 0.807455
486  TRUE 0.071902  1 0.348975
487  TRUE 0.676367  0 0.052082
488  TRUE 0.162360  1 0.863283
489  TRUE 0.580374  0 0.556919
490 FALSE 0.090766  0 0.909955
491  TRUE 0.656099  0 0.761297
492  TRUE 0.357507  1 0.545542
493  TRUE 0.194491  1 0.477882
494  TRUE 0.681934  0 0.347431
495 FALSE 0.030555  0 0.568208
496  TRUE 0.439074  1 0.950769
497  TRUE 0.973690  0 0.261379
498  TRUE 0.412593  1 0.106805
499  TRUE 0.919770  1 0.253758
500  TRUE 0.098764  0 0.180904
> logLik( ss )
'log Lik.' -398 (df=5)
> 
> # estimation with BFGS method
> ssBFGS <- selection( ys ~ xs, yo ~ xo, maxMethod = "BFGS" )
> print( ssBFGS )

Call:
 selection(selection = ys ~ xs, outcome = yo ~ xo, maxMethod = "BFGS") 

Coefficients:
S:(Intercept)           S:xs  O:(Intercept)           O:xo            rho  
       -0.127          3.339         -1.053          1.987          0.811  

> summary( ssBFGS )
--------------------------------------------
Tobit 2 model (sample selection model)
Maximum Likelihood estimation
BFGS maximisation, 77 iterations
Return code 0: successful convergence 
Log-Likelihood: -398 
500 observations (74 censored and 426 observed)
5 free parameters (df = 495)
Probit selection equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -0.127      0.133   -0.96    0.34    
xs             3.339      0.397    8.40  <2e-16 ***
Outcome equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -1.053      0.121   -8.69  <2e-16 ***
xo             1.987      0.225    8.83  <2e-16 ***
Error terms:
    Estimate Std. error t value Pr(> t)    
rho    0.811      0.147    5.51 3.6e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> all.equal( coef( ssBFGS ), coef( ss ), tol = 1e-2 )
[1] TRUE
> all.equal( stdEr( ssBFGS ), stdEr( ss ), tol = 1e-1 )
[1] TRUE
> all.equal( vcov( ssBFGS ), vcov( ss ), tol = 1e-1 )
[1] TRUE
> nobs( ssBFGS )
[1] 500
> nObs( ssBFGS )
[1] 500
> all.equal( fitted( ss ), fitted( ssBFGS ), tol = 1e-2 )
[1] TRUE
> all.equal( fitted( ss, part = "selection" ),
+    fitted( ssBFGS, part = "selection" ), tol = 1e-2 )
[1] TRUE
> all.equal( fitted( ssBFGS ), fitted( ssBFGS, part = "outcome" ) )
[1] TRUE
> all.equal( residuals( ss ), residuals( ssBFGS ), tol = 1e-2 )
[1] TRUE
Warning messages:
1: In Ops.factor(oResponse, fitted(object)) : - not meaningful for factors
2: In Ops.factor(oResponse, fitted(object)) : - not meaningful for factors
> all.equal( residuals( ss, part = "selection" ),
+    residuals( ssBFGS, part = "selection" ), tol = 1e-2 )
[1] TRUE
> all.equal( residuals( ssBFGS ), residuals( ssBFGS, part = "outcome" ) )
[1] TRUE
Warning messages:
1: In Ops.factor(oResponse, fitted(object)) : - not meaningful for factors
2: In Ops.factor(oResponse, fitted(object)) : - not meaningful for factors
> all.equal( model.matrix( ss ), model.matrix( ssBFGS ) )
[1] TRUE
> all.equal( model.matrix( ss, part = "selection" ),
+    model.matrix( ssBFGS, part = "selection" ) )
[1] TRUE
> all.equal( model.matrix( ssBFGS ), model.matrix( ssBFGS, part = "outcome" ) )
[1] TRUE
> all.equal( logLik( ss ), logLik( ssBFGS ), tol = 1e-3 )
[1] TRUE
> 
> # BHHH estimation with equal weights
> we <- rep( 0.7, N )
> ssWe <- selection( ys ~ xs, yo ~ xo, weights = we, steptol = 1e-12 )
> summary( ssWe )
--------------------------------------------
Tobit 2 model (sample selection model)
Maximum Likelihood estimation
BHHH maximisation, 8 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -279 
500 observations (74 censored and 426 observed)
5 free parameters (df = 495)
Probit selection equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -0.127      0.186   -0.69    0.49    
xs             3.342      0.561    5.96 2.5e-09 ***
Outcome equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -1.057      0.178   -5.93 3.0e-09 ***
xo             2.000      0.315    6.35 2.2e-10 ***
Error terms:
    Estimate Std. error t value Pr(> t)    
rho    0.798      0.231    3.46 0.00055 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> all.equal( coef( ssWe ), coef( ss ), tol = 1e-2 )
[1] TRUE
> 
> # BHHH estimation with equal weights
> ssWeBFGS <- selection( ys ~ xs, yo ~ xo, weights = we, maxMethod = "BFGS" )
> summary( ssWeBFGS )
--------------------------------------------
Tobit 2 model (sample selection model)
Maximum Likelihood estimation
BFGS maximisation, 68 iterations
Return code 0: successful convergence 
Log-Likelihood: -279 
500 observations (74 censored and 426 observed)
5 free parameters (df = 495)
Probit selection equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -0.122      0.158   -0.77    0.44    
xs             3.325      0.474    7.02 2.2e-12 ***
Outcome equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -1.049      0.145   -7.24 4.6e-13 ***
xo             1.980      0.269    7.36 1.8e-13 ***
Error terms:
    Estimate Std. error t value Pr(> t)    
rho    0.807      0.181    4.45 8.5e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> all.equal( coef( ssWeBFGS ), coef( ssBFGS ), tol = 1e-2 )
[1] TRUE
> 
> # BHHH estimation with unequal weights
> wu <- 2 * runif( N )
> ssWu <- selection( ys ~ xs, yo ~ xo, weights = wu )
> summary( ssWu )
--------------------------------------------
Tobit 2 model (sample selection model)
Maximum Likelihood estimation
BHHH maximisation, 26 iterations
Return code 1: gradient close to zero
Log-Likelihood: -396 
500 observations (74 censored and 426 observed)
5 free parameters (df = 495)
Probit selection equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -0.111      0.102   -1.08    0.28    
xs             3.355      0.275   12.20  <2e-16 ***
Outcome equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -1.069      0.106   -10.1  <2e-16 ***
xo             1.989      0.181    11.0  <2e-16 ***
Error terms:
    Estimate Std. error t value Pr(> t)    
rho   0.9572     0.0666    14.4  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> 
> # BFGS estimation with unequal weights
> ssWuBFGS <- selection( ys ~ xs, yo ~ xo, weights = wu, maxMethod = "BFGS" )
> summary( ssWuBFGS )
--------------------------------------------
Tobit 2 model (sample selection model)
Maximum Likelihood estimation
BFGS maximisation, 48 iterations
Return code 0: successful convergence 
Log-Likelihood: -396 
500 observations (74 censored and 426 observed)
5 free parameters (df = 495)
Probit selection equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -0.110      0.129   -0.85    0.39    
xs             3.355      0.387    8.66  <2e-16 ***
Outcome equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -1.070      0.120   -8.93  <2e-16 ***
xo             1.992      0.215    9.27  <2e-16 ***
Error terms:
    Estimate Std. error t value Pr(> t)    
rho   0.9568     0.0633    15.1  <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> all.equal( coef( ssWuBFGS ), coef( ssWu ), tol = 1e-2 )
[1] TRUE
> 
> # comparison of estimated coefficients, standard errors, and logLik values
> round( rbind( coef( ss ), coef( ssBFGS ), coef( ssWe ), coef( ssWeBFGS ),
+    coef( ssWu ), coef( ssWuBFGS ) ), 3 )
     (Intercept)   xs (Intercept)   xo   rho
[1,]      -0.130 3.35       -1.05 2.00 0.803
[2,]      -0.127 3.34       -1.05 1.99 0.811
[3,]      -0.127 3.34       -1.06 2.00 0.798
[4,]      -0.122 3.33       -1.05 1.98 0.807
[5,]      -0.111 3.35       -1.07 1.99 0.957
[6,]      -0.110 3.35       -1.07 1.99 0.957
> round( rbind( coef( summary( ss ) )[ , 2 ], coef( summary( ssBFGS ) )[ , 2 ],
+    coef( summary( ssWe ) )[ , 2 ], coef( summary( ssWeBFGS ) )[ , 2 ],
+    coef( summary( ssWu ) )[ , 2 ], coef( summary( ssWuBFGS ) )[ , 2 ] ), 3 )
     (Intercept)    xs (Intercept)    xo   rho
[1,]       0.130 0.392       0.124 0.220 0.158
[2,]       0.133 0.397       0.121 0.225 0.147
[3,]       0.186 0.561       0.178 0.315 0.231
[4,]       0.158 0.474       0.145 0.269 0.181
[5,]       0.102 0.275       0.106 0.181 0.067
[6,]       0.129 0.387       0.120 0.215 0.063
> print( rbind( logLik( ss ), logLik( ssBFGS ), logLik( ssWe ),
+    logLik( ssWeBFGS ), logLik( ssWu ), logLik( ssWuBFGS ) ), digits = 6 )
         [,1]
[1,] -398.197
[2,] -398.192
[3,] -278.739
[4,] -278.737
[5,] -395.660
[6,] -395.660
> 
> # binary outcome NA if unobserved
> yo[ !ys ] <- NA
> print(table(ys, yo, exclude=NULL))
       yo
ys        0   1 <NA>
  FALSE   0   0   74
  TRUE  202 224    0
  <NA>    0   0    0
> ssN <- selection( ys ~ xs, yo ~ xo, steptol = 1e-12 )
> print(summary(ssN))
--------------------------------------------
Tobit 2 model (sample selection model)
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -398 
500 observations (74 censored and 426 observed)
5 free parameters (df = 495)
Probit selection equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -0.130      0.130   -1.00    0.32    
xs             3.347      0.392    8.54  <2e-16 ***
Outcome equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -1.054      0.124   -8.47  <2e-16 ***
xo             1.997      0.220    9.07  <2e-16 ***
Error terms:
    Estimate Std. error t value Pr(> t)    
rho    0.803      0.158    5.07   4e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> all.equal(ss,ssN)
[1] TRUE
> 
> # binary outcome logical
> yo <- yoX > 0 & ys
> print(table(ys, yo, exclude=NULL))
       yo
ys      FALSE TRUE <NA>
  FALSE    74    0    0
  TRUE    202  224    0
  <NA>      0    0    0
> ssL <- selection( ys ~ xs, yo ~ xo, steptol = 1e-12 )
> print(summary(ssL))
--------------------------------------------
Tobit 2 model (sample selection model)
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -398 
500 observations (74 censored and 426 observed)
5 free parameters (df = 495)
Probit selection equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -0.130      0.130   -1.00    0.32    
xs             3.347      0.392    8.54  <2e-16 ***
Outcome equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -1.054      0.124   -8.47  <2e-16 ***
xo             1.997      0.220    9.07  <2e-16 ***
Error terms:
    Estimate Std. error t value Pr(> t)    
rho    0.803      0.158    5.07   4e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> all.equal(ss,ssL)
[1] "Component \"twoStep\": Component \"lm\": Component \"model\": Attributes: < Component \"row.names\": Modes: numeric, character >"              
[2] "Component \"twoStep\": Component \"lm\": Component \"model\": Attributes: < Component \"row.names\": target is numeric, current is character >"
[3] "Component \"twoStep\": Component \"lm\": Component \"model\": Component \"YO\": Modes: numeric, logical"                                       
[4] "Component \"twoStep\": Component \"lm\": Component \"model\": Component \"YO\": names for current but not for target"                          
[5] "Component \"twoStep\": Component \"lm\": Component \"model\": Component \"YO\": target is numeric, current is logical"                         
> 
> # binary outcome logical and NA if unobserved
> yo[ !ys ] <- NA
> print(table(ys, yo, exclude=NULL))
       yo
ys      FALSE TRUE <NA>
  FALSE     0    0   74
  TRUE    202  224    0
  <NA>      0    0    0
> ssLN <- selection( ys ~ xs, yo ~ xo, steptol = 1e-12 )
> print(summary(ssLN))
--------------------------------------------
Tobit 2 model (sample selection model)
Maximum Likelihood estimation
BHHH maximisation, 5 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -398 
500 observations (74 censored and 426 observed)
5 free parameters (df = 495)
Probit selection equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -0.130      0.130   -1.00    0.32    
xs             3.347      0.392    8.54  <2e-16 ***
Outcome equation:
            Estimate Std. error t value Pr(> t)    
(Intercept)   -1.054      0.124   -8.47  <2e-16 ***
xo             1.997      0.220    9.07  <2e-16 ***
Error terms:
    Estimate Std. error t value Pr(> t)    
rho    0.803      0.158    5.07   4e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
--------------------------------------------
> all.equal(ssL,ssLN)
[1] TRUE
> 
> proc.time()
   user  system elapsed 
 67.904   0.168  68.113 
